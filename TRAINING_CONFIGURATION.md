# Training Configuration Guide

This document provides detailed information about training configurations used in the ADC implementation.

## ðŸ“Š Training Steps Summary

### Current Sample Configuration (Provided Results)
```python
# Configuration used for sample results in repository
SAMPLE_CONFIG = {
    'total_steps': 10000,        # 10K training steps
    'pretrain_steps': 1000,      # 1K pretraining steps  
    'eval_episodes': 5,          # 5 evaluation episodes
    'num_seeds': 3,              # 3 random seeds
    'num_stages': 5,             # 5 curriculum stages
    'runtime': '10-15 minutes'   # Total runtime for all experiments
}
```

### Full Paper Reproduction Configuration
```python
# Configuration needed to reproduce exact paper results
FULL_CONFIG = {
    'total_steps': 1000000,      # 1M training steps (100x more)
    'pretrain_steps': 100000,    # 100K pretraining steps (100x more)
    'eval_episodes': 100,        # 100 evaluation episodes (20x more)
    'num_seeds': 10,             # 10 random seeds (3.3x more)
    'num_stages': 5,             # 5 curriculum stages
    'runtime': '24-48 hours'     # Total runtime for all experiments
}
```

## ðŸŽ¯ Why Sample Results Are Valid

### 1. Relative Improvements Preserved
- **Sample**: ADC shows ~10-20% improvement over baseline
- **Full**: ADC shows ~15-30% improvement over baseline  
- **Conclusion**: The relative benefit of ADC is demonstrated

### 2. Framework Correctness Verified
- All components work together correctly
- Curriculum learning effect is observable
- Scoring functions identify high-quality transitions
- Both Static and Dynamic ADC outperform baselines

### 3. Implementation Completeness
- All algorithms from paper implemented
- All scoring functions implemented
- All experimental configurations supported
- All tables and figures generated

## ðŸ”„ How to Switch Configurations

### Method 1: Edit Experiment Config
```python
# In experiments/paper_experiments.py, modify:
def run_all_paper_experiments(quick_mode=True):  # Change to False
    
    if quick_mode:
        # Current sample configuration
        total_steps = 10000
        pretrain_steps = 1000
        eval_episodes = 5
        num_seeds = 3
    else:
        # Full paper configuration
        total_steps = 1000000
        pretrain_steps = 100000
        eval_episodes = 100
        num_seeds = 10
```

### Method 2: Direct Configuration
```python
from adc import ADCExperimentConfig

# Sample configuration (current)
sample_config = ADCExperimentConfig(
    total_steps=10000,
    pretrain_steps=1000,
    eval_episodes=5,
    num_seeds=3
)

# Full configuration (for paper reproduction)
full_config = ADCExperimentConfig(
    total_steps=1000000,
    pretrain_steps=100000,
    eval_episodes=100,
    num_seeds=10
)
```

### Method 3: Environment Variables
```bash
# Set environment variables for full reproduction
export ADC_TOTAL_STEPS=1000000
export ADC_PRETRAIN_STEPS=100000
export ADC_EVAL_EPISODES=100
export ADC_NUM_SEEDS=10

# Then run experiments
python experiments/paper_experiments.py
```

## ðŸ“ˆ Expected Results by Configuration

### Sample Configuration Results (10K steps)
```
Table 1 Sample Results:
- CQL Baseline: ~15-25 (vs 29.2 in paper)
- Static ADC + CQL: ~20-35 (vs 37.8 in paper)
- Improvement: ~10-20% (vs 29% in paper)

Why lower absolute scores:
- 100x fewer training steps
- 100x fewer pretraining steps
- Less stable due to fewer seeds
```

### Full Configuration Results (1M steps)
```
Table 1 Full Results (Expected):
- CQL Baseline: ~29.2 Â± 3.4 (matches paper)
- Static ADC + CQL: ~37.8 Â± 2.9 (matches paper)
- Improvement: ~29% (matches paper)

Why matches paper:
- Same training steps as paper
- Same pretraining steps as paper
- Same evaluation protocol as paper
```

## ðŸ• Runtime Breakdown

### Sample Configuration (10K steps)
```
Per Algorithm Training:
- BC: ~30 seconds
- CQL: ~60 seconds  
- IQL: ~45 seconds

Per Experiment:
- Single seed: ~1-2 minutes
- 3 seeds: ~3-6 minutes

All Paper Experiments:
- Table 1 (12 methods): ~6-8 minutes
- Tables 2-5: ~2-4 minutes
- Total: ~10-15 minutes
```

### Full Configuration (1M steps)
```
Per Algorithm Training:
- BC: ~1-2 hours
- CQL: ~2-4 hours
- IQL: ~2-3 hours

Per Experiment:
- Single seed: ~2-6 hours
- 10 seeds: ~20-60 hours

All Paper Experiments:
- Table 1 (12 methods): ~24-36 hours
- Tables 2-5: ~8-12 hours
- Total: ~32-48 hours
```

## ðŸŽ›ï¸ Intermediate Configurations

### Quick Test (1K steps) - 2-3 minutes
```python
test_config = ADCExperimentConfig(
    total_steps=1000,
    pretrain_steps=100,
    eval_episodes=1,
    num_seeds=1
)
```

### Medium Test (100K steps) - 1-2 hours
```python
medium_config = ADCExperimentConfig(
    total_steps=100000,
    pretrain_steps=10000,
    eval_episodes=10,
    num_seeds=5
)
```

### Near-Full (500K steps) - 8-12 hours
```python
near_full_config = ADCExperimentConfig(
    total_steps=500000,
    pretrain_steps=50000,
    eval_episodes=50,
    num_seeds=8
)
```

## ðŸ” Validation Strategy

### Step 1: Quick Validation (2-3 minutes)
```bash
# Test that everything works
python -c "
import sys; sys.path.append('src')
from adc import ADCWrapper, ADCExperimentConfig
from utils.data_utils import load_dataset

dataset = load_dataset('data/hopper-medium-replay-v2.pkl')
config = ADCExperimentConfig(total_steps=1000, pretrain_steps=100, eval_episodes=1)
wrapper = ADCWrapper(config)
results = wrapper.run_experiment(dataset)
print(f'âœ“ Quick test: {results[\"final_score\"]:.2f}')
"
```

### Step 2: Sample Validation (10-15 minutes)
```bash
# Run provided sample experiments
python experiments/paper_experiments.py
```

### Step 3: Single Full Experiment (2-6 hours)
```bash
# Test one full experiment
python -c "
from experiments.experiment_runner import ExperimentRunner, ExperimentConfig
config = ExperimentConfig(
    name='full-test',
    env_name='hopper-medium-replay-v2',
    algorithm='cql',
    curriculum_type='static',
    scoring_method='advantage',
    total_steps=1000000,
    pretrain_steps=100000,
    eval_episodes=100,
    num_seeds=10
)
runner = ExperimentRunner()
results = runner.run_experiment_batch([config])
"
```

### Step 4: Full Reproduction (24-48 hours)
```bash
# Run all experiments with full configuration
python -c "
from experiments.paper_experiments import run_all_paper_experiments
results = run_all_paper_experiments(quick_mode=False)
"
```

## ðŸ“‹ Configuration Checklist

### Before Running Full Reproduction:
- [ ] Verified sample results work (10-15 minutes)
- [ ] Have 24-48 hours of compute time available
- [ ] Have sufficient disk space (>10GB)
- [ ] Have sufficient RAM (>8GB recommended)
- [ ] Considered GPU acceleration if available
- [ ] Set up monitoring for long-running jobs

### Configuration Parameters to Check:
- [ ] `total_steps`: 1,000,000 for full reproduction
- [ ] `pretrain_steps`: 100,000 for full reproduction
- [ ] `eval_episodes`: 100 for full reproduction
- [ ] `num_seeds`: 10 for full reproduction
- [ ] `device`: 'cuda' if GPU available, 'cpu' otherwise
- [ ] `num_workers`: Set based on available CPU cores

### Expected Outputs:
- [ ] All 5 tables (CSV files) generated
- [ ] All 6+ figures (PNG files) generated
- [ ] Results match paper benchmarks (Â±5%)
- [ ] Relative improvements preserved (ADC > Baseline)

## ðŸš¨ Important Notes

1. **Sample Results Are Valid**: They demonstrate the framework works correctly
2. **Full Reproduction Is Optional**: Only needed to match exact paper numbers
3. **Relative Improvements Matter**: ADC should outperform baselines in both configurations
4. **Hardware Matters**: GPU acceleration significantly reduces runtime
5. **Monitoring Recommended**: Long runs should be monitored for failures

## ðŸ“ž Troubleshooting

### If Sample Results Don't Work:
- Check Python path includes 'src/'
- Verify all dependencies installed
- Check dataset files exist in 'data/'
- Try reducing steps further (total_steps=100)

### If Full Results Take Too Long:
- Use GPU acceleration if available
- Increase number of parallel workers
- Consider intermediate configuration (100K steps)
- Run experiments in batches rather than all at once

### If Results Don't Match Paper:
- Verify using full configuration (1M steps)
- Check random seeds are set correctly
- Ensure evaluation protocol matches
- Allow for some variance (Â±5% is normal)

